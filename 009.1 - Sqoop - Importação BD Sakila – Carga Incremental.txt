Sqoop - Importação BD Sakila – Carga Incremental

Realizar com uso do MySQL

1. Criar a tabela cp_rental_append, contendo a cópia da tabela rental com os campos rental_id e rental_date

create table cp_rental_append
select rental_id, rental_date from rental;

2 .Criar a tabela cp_rental_id e cp_rental_date, contendo a cópia da tabela cp_rental_append

create table cp_rental_id
select * from cp_rental_append;

create table cp_rental_date
select * from cp_rental_append;

Realizar com uso do Sqoop - Importações no warehouse /user/hive/warehouse/db_test3 e visualizar no HDFS

3. Importar as tabelas cp_rental_append, cp_rental_id e cp_rental_date com 1 mapeador

sqoop import --table cp_rental_append \
--connect jdbc:mysql://database/sakila \
--username root \
--password secret \
--warehouse-dir /user/hive/warehouse/db_test3 -m 1

sqoop import --table cp_rental_id \
--connect jdbc:mysql://database/sakila \
--username root \
--password secret \
--warehouse-dir /user/hive/warehouse/db_test3 -m 1

sqoop import --table cp_rental_date \
--connect jdbc:mysql://database/sakila \
--username root \
--password secret \
--warehouse-dir /user/hive/warehouse/db_test3 -m 1

Realizar com uso do MySQL

4. Executar o sql /db-sql/sakila/insert_rental.sql no container do database

$ docker exec -it database bash

$ cd /db-sql/sakila

$ mysql -psecret < insert_rental.sql

use sakila;
DROP TABLE IF EXISTS cp_rental_append, cp_rental_date, cp_rental_id;
create table cp_rental_append select rental_id, rental_date from rental;
create table cp_rental_date select * from cp_rental_append;
create table cp_rental_id select * from cp_rental_date;

insert into cp_rental_date values(16048,'2005-08-23 22:30:00');
insert into cp_rental_date values(16049,'2005-08-23 22:40:00');
insert into cp_rental_date values(16050,'2005-08-23 22:52:50');
insert into cp_rental_date values(16051,'2005-08-23 22:54:30');
insert into cp_rental_date values(16052,'2005-08-23 22:55:20');
insert into cp_rental_date values(16054,'2005-08-23 22:57:40');
insert into cp_rental_date values(16053,'2005-08-23 22:56:10');
insert into cp_rental_date values(16055,'2005-08-23 22:59:20');

insert into cp_rental_id values(16048,'2005-08-23 22:30:00');
insert into cp_rental_id values(16049,'2005-08-23 22:40:00');
insert into cp_rental_id values(16050,'2005-08-23 22:52:50');
insert into cp_rental_id values(16051,'2005-08-23 22:54:30');
insert into cp_rental_id values(16052,'2005-08-23 22:55:20');
insert into cp_rental_id values(16054,'2005-08-23 22:57:40');
insert into cp_rental_id values(16053,'2005-08-23 22:56:10');
insert into cp_rental_id values(16055,'2005-08-23 22:59:20');

select * from cp_rental_date ORDER By rental_date DESC limit 10;

select * from cp_rental_id ORDER By rental_id DESC limit 10;

Realizar com uso do Sqoop - Importações no warehouse /user/hive/warehouse/db_test3 e visualizar no HDFS

5. Atualizar a tabela cp_rental_append no HDFS anexando os novos arquivos

sqoop import --table cp_rental_append \
--connect jdbc:mysql://database/sakila \
--username root \
--password secret \
--warehouse-dir /user/hive/warehouse/db_test3 -m 1
--append

6. Atualizar a tabela cp_rental_id no HDFS de acordo com o último registro de rental_id, adicionando apenas os novos dados.

sqoop import --table cp_rental_id \
--connect jdbc:mysql://database/sakila \
--username root \
--password secret \
--warehouse-dir /user/hive/warehouse/db_test3 -m 1
--incremental append \
--check-column rental_id \
--last-value 16049

7. Atualizar a tabela cp_rental_date no HDFS de acordo com o último registro de rental_date, atualizando os registros a partir desta data.

sqoop import --table cp_rental_date \
--connect jdbc:mysql://database/sakila \
--username root \
--password secret \
--warehouse-dir /user/hive/warehouse/db_test3 -m 1
--incremental lastmodified\
--merge-key rental_id \
--check-column rental_date \
--last-value '2005-08-23 22:50:12.0'








